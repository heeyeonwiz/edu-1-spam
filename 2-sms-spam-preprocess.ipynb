{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. 영어 스팸 문자 데이터 전처리(PreProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1) 데이터 로드 : dataframe (pandas module 사용)  \n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset  \n",
    "train과 test가 별도로 나누어 지지 않았고, spam.csv 하나만 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # 경고 메시지 안보이게 설정\n",
    "\n",
    "import gc\n",
    "gc.collect() # garbage collector : 메모리 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "전역 변수 중 일부(디렉토리 이름과 파일 이름 등)는 대문자로  \n",
    "나머지 변수는 소문자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로는 단순히 문자열 연결보다는 os.path.join()을 사용하는 것이 좋음 \n",
    "DATA_DIR = 'data'\n",
    "RAW_DATA_FILE = 'spam.csv'\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR, RAW_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(RAW_DATA_PATH, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측 데이터는 없었으므로 바로 중복 데이터 제거\n",
    "dedupe_raw_df = raw_df.drop_duplicates('v2', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) 라벨 인코딩 : ham을 0으로 spam을 1로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "dedupe_raw_df['label'] = le.fit_transform(dedupe_raw_df['v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3) 데이터 분리 : training data와 test data로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "\n",
    "test_size = 0.1 # split_ratio = ['0.9', '0.1']\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(dedupe_raw_df['v2'], dedupe_raw_df['label'], test_size=0.1, random_state=434)\n",
    "train_df, test_df = train_test_split(dedupe_raw_df, test_size=0.1, random_state=434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (가) split한 데이터를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_df))\n",
    "print(type(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_train_df = pd.merge(train_x, train_y, left_index=True, right_index=True, how='left')\n",
    "# merged_test_df = pd.merge(test_x, test_y, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "SPLITED_TRAIN_FILE = 'splited_train.csv'\n",
    "SPLITED_TEST_FILE = 'splited_test.csv'\n",
    "\n",
    "SPLITED_TRAIN_PATH = os.path.join(PROCESSED_DATA_DIR, SPLITED_TRAIN_FILE)\n",
    "SPLITED_TEST_PATH = os.path.join(PROCESSED_DATA_DIR, SPLITED_TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(SPLITED_TRAIN_PATH, index = False)\n",
    "test_df.to_csv(SPLITED_TEST_PATH, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4) 토큰화(Tokenization) : 문장을 단어 조각으로 분리하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df['v2']\n",
    "train_y = train_df['label']\n",
    "test_x = test_df['v2']\n",
    "test_y = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(가) 단어 사전을 만들기 위해 첫 번째 토큰화 작업\n",
    "  - 디폴트 값으로 토큰화하여 결과를 관찰하고 참고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_x)  # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합(vocabulary)을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석한 결과를 보기 : 전체 어휘의 숫자와 희귀단어 분석\n",
    "\n",
    "threshold = 3 # 등장 빈도 하한, 이 수보다 작은 빈도를 나타내는 어휘는 희귀 단어로 정한다.\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 어휘 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(나) 앞에서 만든 단어 사전을 이용하여 새롭게 토큰화\n",
    "  - Tokenizer()를 초기화하고 다시 토큰화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 2534\n",
    "\n",
    "oov_tok = \"<OOV>\" # Out-Of-Vocabulary 토큰\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 밸류로 정렬, 사용 빈도가 많은 어휘부터 볼 수 있다.\n",
    "import operator\n",
    "\n",
    "sorted_dict = sorted(tokenizer.word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tok = \"<OOV>\" \n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words \n",
    "tot_words = len(tokenizer.word_index)\n",
    "print('There are %s unique tokens in training data. ' % tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_FILE = 'tokenized.json'\n",
    "TOKENIZED_PATH = os.path.join(DATA_DIR, TOKENIZED_FILE)\n",
    "\n",
    "import json\n",
    "\n",
    "tok_json = tokenizer.to_json()\n",
    "# with io.open(DATA_IN_DIR + 'tokenizer_' + ver + '.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "json.dump(tok_json, open(TOKENIZED_PATH, 'w'), ensure_ascii=False)\n",
    "\n",
    "# 읽는 방법 : 아래 2 가지 방법 중 한 가지 방법으로 읽어 올 수 있다.\n",
    "'''\n",
    "tok_configs = None\n",
    "with open(tokenized_file, 'r') as f:\n",
    "    tok_configs = json.load(f)\n",
    "'''\n",
    "# keras.preprocessing.text.tokenizer_from_json(json_string)\n",
    "'''\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5) 시퀀싱과 패딩(Sequencing and Padding)  \n",
    "토큰화 된 데이터를 숫자(인덱스) 시퀀스로 바꾸고 같은 길이의 시퀀스로 만들기 위해 패딩을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x # split할 때 순서가 바뀌었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = 20 \n",
    "trunc_type = \"post\" \n",
    "padding_type = \"post\" \n",
    "\n",
    "# padding : max_len에 길이를 맞춘다. 길면 자르고 부족하면 0으로 채운다.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "train_padded = pad_sequences (train_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[2901] # 첫 번째 데이터의 인덱스가 2901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences[0] # 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.sequences_to_texts([train_sequences[0]]) # 역으로 문장 확인\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of train tensor\n",
    "print('Shape of train tensor: ', train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before padding\n",
    "len(train_sequences[0]), len(train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After padding\n",
    "len(train_padded[0]), len(train_padded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6) 전처리(PreProcessed)된 Data를 파일에 저장\n",
    "\n",
    "dataframe의 객체를 그대로 저장하려면, (list, dict 등 포함) pickle로 저장한다.    \n",
    "to_pickle, read_pickle도 사용 가능 :   \n",
    "https://wikidocs.net/8929  \n",
    "https://tariat.tistory.com/739  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "PROCESSED_DATA_FILE = \"train_padded.p\"\n",
    "PROCESSED_LABEL_FILE = \"train_label.p\"\n",
    "\n",
    "PROCESSED_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_DATA_FILE)\n",
    "PROCESSED_LABEL_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장, pickle을 이용하면 python의 데이터 타입을 그대로 저장할 수 있음.\n",
    "import pickle\n",
    "\n",
    "with open(PROCESSED_DATA_PATH, \"wb\") as file:\n",
    "    pickle.dump(train_padded, file)\n",
    "    \n",
    "with open(PROCESSED_LABEL_PATH, \"wb\") as file:\n",
    "    pickle.dump(train_y, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일 로드 확인\n",
    "with open(PROCESSED_DATA_PATH, \"rb\" ) as file:\n",
    "    train_padded_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7) Test dataset 전처리\n",
    "\n",
    "트레이닝 후 정확도를 측정하기 위해 사용하기 위해 테스트 데이터셋도 전처리해 둔다.\n",
    "  1. test dataset load : split 해서 얻은 데이터셋 사용\n",
    "  2. 토큰화\n",
    "  3. 시퀀스 만들기와 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of testing tensor: ', test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "PROCESSED_TEST_DATA_FILE = \"test_padded.p\"\n",
    "PROCESSED_TEST_LABEL_FILE = \"test_label.p\"\n",
    "\n",
    "PROCESSED_TEST_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_TEST_DATA_FILE)\n",
    "PROCESSED_TEST_LABEL_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_TEST_LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장, pickle을 이용하면 python의 데이터 타입을 그대로 저장할 수 있음.\n",
    "import pickle\n",
    "\n",
    "with open(PROCESSED_TEST_DATA_PATH, \"wb\") as file:\n",
    "    pickle.dump(test_padded, file)\n",
    "    \n",
    "with open(PROCESSED_TEST_LABEL_PATH, \"wb\") as file:\n",
    "    pickle.dump(test_y, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
